from __future__ import annotations
import re
import operator
from typing import Any, List, Dict, Annotated, TypedDict

import torch
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# -------------------------
# 1) Local LLM Setup
# -------------------------
print("Setting up local LLM...")
# Using a slightly larger model for potentially better instruction following
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/Qwen3-0.6B-unsloth-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
    dtype=None,
    device_map="auto",
)
tokenizer = get_chat_template(tokenizer, chat_template="qwen3")


class CustomUnslothChatModel(BaseChatModel):
    """Minimal LangChain-compatible wrapper around an Unsloth HF chat model."""
    model: Any
    tokenizer: Any

    def _normalize_turns(self, formatted: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Ensures roles alternate user/assistant, folding system prompts into the next user message."""
        norm: List[Dict[str, str]] = []
        for msg in formatted:
            role, content = msg["role"], (msg.get("content") or "").strip()
            if not content: continue

            # Fold system message into the next user message for models that don't have a distinct system role
            if role == "system":
                if norm and norm[-1]["role"] == "user":
                    norm[-1]["content"] = f"[System]\n{content}\n\n[User]\n{norm[-1]['content']}"
                else:
                    norm.append({"role": "user", "content": f"[System]\n{content}"})
            elif norm and norm[-1]["role"] == role:  # Merge consecutive messages of the same role
                norm[-1]["content"] += f"\n\n{content}"
            else:
                norm.append({"role": role, "content": content})
        return norm

    def _generate(
            self,
            messages: List[BaseMessage],
            stop: List[str] | None = None,
            run_manager: CallbackManagerForLLMRun | None = None,
            **kwargs: Any,
    ) -> ChatResult:
        # Map LangChain messages to a generic dictionary format
        formatted_msgs = [{"role": m.type, "content": m.content} for m in messages]
        normalized_msgs = self._normalize_turns(formatted_msgs)

        # Apply the chat template
        prompt = self.tokenizer.apply_chat_template(
            normalized_msgs, tokenize=False, add_generation_prompt=True
        )
        inputs = self.tokenizer([prompt], return_tensors="pt").to(self.model.device)

        # Set generation parameters, allowing overrides from kwargs
        gen_kwargs = {
            "max_new_tokens": 200,
            "do_sample": True,
            "temperature": 0.6,
            "top_p": 0.9,
            "use_cache": True,
            **kwargs,
        }

        outputs = self.model.generate(**inputs, **gen_kwargs)
        decoded = self.tokenizer.batch_decode(
            outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True
        )[0].strip()

        # Fallback response if generation is empty
        if not decoded:
            decoded = "Thank you for the information. I have updated your case notes. What other symptoms have you noticed?"

        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=decoded))])

    @property
    def _llm_type(self) -> str:
        return "custom_unsloth_chat_model"


chat_model = CustomUnslothChatModel(model=model, tokenizer=tokenizer)


# -------------------------
# 2) Agent State and Tools
# -------------------------
class AgentState(TypedDict, total=False):
    """
    Defines the state of our agent. It's carried over between turns.

    Attributes:
        messages: The history of the conversation. `operator.add` ensures new messages are appended.
        patient_info: A dictionary to store structured data extracted from the conversation.
        output: The last message generated by the AI to be shown to the user.
    """
    messages: Annotated[List[BaseMessage], operator.add]
    patient_info: Dict[str, Any]
    output: str


# Mock databases/tools for the agent to use
PHENOTYPE_DB = {
    "marfan syndrome": "FBN1",
    "cystic fibrosis": "CFTR",
    "sickle cell anemia": "HBB",
    "migraine with aura": "CACNA1A, ATP1A2, SCN1A",
}


def lookup_gene_for_phenotype(phenotype: str) -> str:
    """Looks up a gene associated with a given phenotype."""
    return PHENOTYPE_DB.get(phenotype.lower().strip(), "Gene not found in database.")


def score_variant_pathogenicity(variant: str) -> Dict[str, Any]:
    """Generates a mock pathogenicity score for a genetic variant."""
    score = (sum(ord(c) for c in variant if c.isalnum()) % 100) / 100.0
    classification = "Pathogenic" if score > 0.8 else "Likely Pathogenic" if score > 0.6 else "VUS"
    return {"variant": variant, "score": f"{score:.2f}", "classification": classification}


# -------------------------
# 3) Graph Nodes
# -------------------------
def update_patient_info(state: AgentState) -> Dict[str, Any]:
    """Node to extract and accumulate structured patient info from the conversation."""
    # Start with existing info to ensure continuity
    patient_info = state.get("patient_info", {}).copy()
    # Only scan the most recent human message to avoid redundant processing
    last_human_message = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)

    if last_human_message:
        txt = last_human_message.lower()
        # Extract age
        age_match = re.search(r"(\b\d{1,3}\b)\s*(?:y/o|yo|year[s]?-?old)", txt)
        if age_match:
            patient_info["age"] = int(age_match.group(1))

        # Extract symptoms using a keyworded pattern
        symptoms_match = re.search(r"symptom[s]?\s*:\s*(.*)", txt)
        if symptoms_match:
            symptoms = [s.strip() for s in symptoms_match.group(1).split(',') if s.strip()]
            current_symptoms = set(patient_info.get("symptoms", []))
            current_symptoms.update(symptoms)
            patient_info["symptoms"] = sorted(list(current_symptoms))

    return {"patient_info": patient_info}


def branch_enrich(state: AgentState) -> Dict[str, Any]:
    """Node to perform lookups (e.g., phenotype->gene) based on the latest user query."""
    patient_info = state.get("patient_info", {}).copy()
    last_human_message = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), "")

    # If user asks about genes for a condition, use the latest symptom as the phenotype
    if any(keyword in last_human_message.lower() for keyword in ["gene", "genetic", "linked to"]):
        if "symptoms" in patient_info and patient_info["symptoms"]:
            phenotype = patient_info["symptoms"][-1]  # Assume the most recent symptom is the subject
            patient_info["gene_suggestion"] = lookup_gene_for_phenotype(phenotype)

    # If user provides a variant, score it
    variant_match = re.search(r"variant\s+([a-zA-Z0-9]+\s+p\.[a-zA-Z0-9*]+)", last_human_message, re.IGNORECASE)
    if variant_match:
        variant = variant_match.group(1)
        patient_info["variant_assessment"] = score_variant_pathogenicity(variant)

    return {"patient_info": patient_info}


def call_model(state: AgentState) -> Dict[str, Any]:
    """Node that calls the LLM to generate a response based on the accumulated state."""
    pinfo = state.get("patient_info", {})
    print("patient info", pinfo)
    # Clear previous genetic lookups so they don't persist unless re-triggered
    pinfo.pop("gene_suggestion", None)
    pinfo.pop("variant_assessment", None)

    system_prompt = (
        "You are a helpful clinical support assistant for educational purposes. "
        "Your goal is to maintain and refine a differential diagnosis based on patient information. "
        "Summarize the patient's case, list top 3 differential diagnoses, ask 2 clarifying questions, "
        "and note one key red flag. If genetic information is available in the current context, "
        "incorporate it. Keep your response concise and structured."
    )

    # Construct a clear prompt with the current patient data
    prompt_context = f"Current patient information:\n{pinfo}\n\nBased on this, provide your assessment."
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt_context)
    ]

    ai_response = chat_model.invoke(messages, temperature=0.5)

    # The output of this node will be a new AI message and the final user-facing text
    return {"messages": [ai_response], "output": ai_response.content}


# -------------------------
# 4) Graph Definition
# -------------------------
print("Defining agent graph...")
workflow = StateGraph(AgentState)

# Add nodes to the graph
workflow.add_node("update_info", update_patient_info)
workflow.add_node("branch_enrich", branch_enrich)
workflow.add_node("agent", call_model)

# Define the flow of control
workflow.set_entry_point("update_info")
workflow.add_edge("update_info", "branch_enrich")
workflow.add_edge("branch_enrich", "agent")
workflow.add_edge("agent", END)

# Use MemorySaver for in-memory persistence across turns
checkpointer = MemorySaver()

# ----------------------------------------------------
# 5) Compile the Graph (This is our Agent)
# ----------------------------------------------------
# The compiled graph, with its checkpointer, is the complete stateful agent.
# We no longer need the RunnableWithMessageHistory wrapper.
app = workflow.compile(checkpointer=checkpointer)

# -------------------------
# 6) Demo Interaction
# -------------------------
if __name__ == "__main__":
    # A unique ID for the conversation thread
    session_id = "patient_case_009"
    config = {"configurable": {"thread_id": session_id}}

    print("\n--- Patient Consultation (Educational Demo) ---\n")

    # Turn 1: User reports initial symptom
    print("[Turn 1] User reports symptom")
    user_input = "Hello, I have a severe recurring headache. Symptoms: Strong headache"
    # We invoke the app directly, passing the new message. The checkpointer handles loading/saving history.
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Turn 2: User provides age
    print("\n[Turn 2] User provides age")
    user_input = "I'm 35 years-old."
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Turn 3: User asks about genetics (triggers the 'branch_enrich' logic)
    print("\n[Turn 3] User asks about genetics")
    user_input = "Are there any genes linked to this condition?"
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Turn 4: User supplies a specific variant for assessment
    print("\n[Turn 4] User supplies a variant")
    user_input = "What about variant CACNA1A p.Arg583Gln?"
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Inspect the final state to verify that memory was maintained
    print("\n--- Final Persisted patient_info ---")
    final_state = checkpointer.get(config)
    if final_state:
        print(final_state['channel_values'].get('patient_info'))
    else:
        print("No final state found for the session.")
