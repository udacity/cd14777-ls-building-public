from __future__ import annotations
import re
import operator
from typing import Any, List, Dict, Annotated, TypedDict

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# -------------------------
# 1) Local LLM Setup
# -------------------------
print("Setting up local LLM...")
# Using a small model that works well without quantization
model_name = "Qwen/Qwen3-0.6B"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # Use float16 for efficiency without quantization
    device_map="auto",
    trust_remote_code=True,
)


class CustomChatModel(BaseChatModel):
    """Minimal LangChain-compatible wrapper around a Hugging Face chat model."""
    model: Any
    tokenizer: Any

    def _format_messages(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """Convert LangChain messages to the format expected by the chat template."""
        formatted = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                role = "system"
            elif isinstance(msg, HumanMessage):
                role = "user"
            elif isinstance(msg, AIMessage):
                role = "assistant"
            else:
                role = "user"  # Default fallback

            formatted.append({"role": role, "content": msg.content})

        return formatted

    def _generate(
            self,
            messages: List[BaseMessage],
            stop: List[str] | None = None,
            run_manager: CallbackManagerForLLMRun | None = None,
            **kwargs: Any,
    ) -> ChatResult:
        # Convert messages to chat format
        formatted_msgs = self._format_messages(messages)

        # Apply the chat template
        prompt = self.tokenizer.apply_chat_template(
            formatted_msgs,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer([prompt], return_tensors="pt").to(self.model.device)

        # Set generation parameters, allowing overrides from kwargs
        gen_kwargs = {
            "max_new_tokens": 200,
            "do_sample": True,
            "temperature": 0.6,
            "top_p": 0.9,
            "use_cache": True,
            "pad_token_id": self.tokenizer.eos_token_id,
            **kwargs,
        }

        with torch.no_grad():
            outputs = self.model.generate(**inputs, **gen_kwargs)

        decoded = self.tokenizer.batch_decode(
            outputs[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )[0].strip()

        # Fallback response if generation is empty
        if not decoded:
            decoded = "Thank you for the information. I have updated your case notes. What other symptoms have you noticed?"

        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=decoded))])

    @property
    def _llm_type(self) -> str:
        return "custom_chat_model"


chat_model = CustomChatModel(model=model, tokenizer=tokenizer)


# -------------------------
# 2) Agent State and Tools
# -------------------------
class AgentState(TypedDict, total=False):
    """
    Defines the state of our agent. It's carried over between turns.

    Attributes:
        messages: The history of the conversation. `operator.add` ensures new messages are appended.
        patient_info: A dictionary to store structured data extracted from the conversation.
        output: The last message generated by the AI to be shown to the user.
    """
    messages: Annotated[List[BaseMessage], operator.add]
    patient_info: Dict[str, Any]
    output: str


# Mock databases/tools for the agent to use
PHENOTYPE_DB = {
    "marfan syndrome": "FBN1",
    "cystic fibrosis": "CFTR",
    "sickle cell anemia": "HBB",
    "migraine with aura": "CACNA1A, ATP1A2, SCN1A",
}


def lookup_gene_for_phenotype(phenotype: str) -> str:
    """Looks up a gene associated with a given phenotype."""
    return PHENOTYPE_DB.get(phenotype.lower().strip(), "Gene not found in database.")


def score_variant_pathogenicity(variant: str) -> Dict[str, Any]:
    """Generates a mock pathogenicity score for a genetic variant."""
    score = (sum(ord(c) for c in variant if c.isalnum()) % 100) / 100.0
    classification = "Pathogenic" if score > 0.8 else "Likely Pathogenic" if score > 0.6 else "VUS"
    return {"variant": variant, "score": f"{score:.2f}", "classification": classification}


# -------------------------
# 3) Graph Nodes
# -------------------------
def update_patient_info(state: AgentState) -> Dict[str, Any]:
    """Node to extract and accumulate structured patient info from the conversation."""
    # Start with existing info to ensure continuity
    patient_info = state.get("patient_info", {}).copy()
    # Only scan the most recent human message to avoid redundant processing
    last_human_message = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)

    if last_human_message:
        txt = last_human_message.lower()
        # Extract age
        age_match = re.search(r"(\b\d{1,3}\b)\s*(?:y/o|yo|year[s]?-?old)", txt)
        if age_match:
            patient_info["age"] = int(age_match.group(1))

        # Extract symptoms using a keyworded pattern
        symptoms_match = re.search(r"symptom[s]?\s*:\s*(.*)", txt)
        if symptoms_match:
            symptoms = [s.strip() for s in symptoms_match.group(1).split(',') if s.strip()]
            current_symptoms = set(patient_info.get("symptoms", []))
            current_symptoms.update(symptoms)
            patient_info["symptoms"] = sorted(list(current_symptoms))

    return {"patient_info": patient_info}


def branch_enrich(state: AgentState) -> Dict[str, Any]:
    """Node to perform lookups (e.g., phenotype->gene) based on the latest user query."""
    patient_info = state.get("patient_info", {}).copy()
    last_human_message = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), "")

    # If user asks about genes for a condition, use the latest symptom as the phenotype
    if any(keyword in last_human_message.lower() for keyword in ["gene", "genetic", "linked to"]):
        if "symptoms" in patient_info and patient_info["symptoms"]:
            phenotype = patient_info["symptoms"][-1]  # Assume the most recent symptom is the subject
            patient_info["gene_suggestion"] = lookup_gene_for_phenotype(phenotype)

    # If user provides a variant, score it
    variant_match = re.search(r"variant\s+([a-zA-Z0-9]+\s+p\.[a-zA-Z0-9*]+)", last_human_message, re.IGNORECASE)
    if variant_match:
        variant = variant_match.group(1)
        patient_info["variant_assessment"] = score_variant_pathogenicity(variant)

    return {"patient_info": patient_info}


def call_model(state: AgentState) -> Dict[str, Any]:
    """Node that calls the LLM to generate a response based on the accumulated state."""
    pinfo = state.get("patient_info", {})
    print("patient info", pinfo)
    # Clear previous genetic lookups so they don't persist unless re-triggered
    pinfo.pop("gene_suggestion", None)
    pinfo.pop("variant_assessment", None)

    system_prompt = (
        "You are a helpful clinical support assistant for educational purposes. "
        "Your goal is to maintain and refine a differential diagnosis based on patient information. "
        "Summarize the patient's case, list top 3 differential diagnoses, ask 2 clarifying questions, "
        "and note one key red flag. If genetic information is available in the current context, "
        "incorporate it. Keep your response concise and structured."
    )

    # Construct a clear prompt with the current patient data
    prompt_context = f"Current patient information:\n{pinfo}\n\nBased on this, provide your assessment."
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt_context)
    ]

    ai_response = chat_model.invoke(messages, temperature=0.5)

    # The output of this node will be a new AI message and the final user-facing text
    return {"messages": [ai_response], "output": ai_response.content}


# -------------------------
# 4) Graph Definition
# -------------------------
print("Defining agent graph...")
workflow = StateGraph(AgentState)

# Add nodes to the graph
workflow.add_node("update_info", update_patient_info)
workflow.add_node("branch_enrich", branch_enrich)
workflow.add_node("agent", call_model)

# Define the flow of control
workflow.set_entry_point("update_info")
workflow.add_edge("update_info", "branch_enrich")
workflow.add_edge("branch_enrich", "agent")
workflow.add_edge("agent", END)

# Use MemorySaver for in-memory persistence across turns
checkpointer = MemorySaver()

# ----------------------------------------------------
# 5) Compile the Graph (This is our Agent)
# ----------------------------------------------------
app = workflow.compile(checkpointer=checkpointer)

# -------------------------
# 6) Demo Interaction
# -------------------------
if __name__ == "__main__":
    # A unique ID for the conversation thread
    session_id = "patient_case_009"
    config = {"configurable": {"thread_id": session_id}}

    print("\n--- Patient Consultation (Educational Demo) ---\n")

    # Turn 1: User reports initial symptom
    print("[Turn 1] User reports symptom")
    user_input = "Hello, I have a severe recurring headache. Symptoms: Strong headache"
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Turn 2: User provides age
    print("\n[Turn 2] User provides age")
    user_input = "I'm 35 years-old."
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Turn 3: User asks about genetics (triggers the 'branch_enrich' logic)
    print("\n[Turn 3] User asks about genetics")
    user_input = "Are there any genes linked to this condition?"
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Turn 4: User supplies a specific variant for assessment
    print("\n[Turn 4] User supplies a variant")
    user_input = "What about variant CACNA1A p.Arg583Gln?"
    out = app.invoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    print("AI:", out.get("output", ""))

    # Inspect the final state to verify that memory was maintained
    print("\n--- Final Persisted patient_info ---")
    final_state = checkpointer.get(config)
    if final_state:
        print(final_state['channel_values'].get('patient_info'))
    else:
        print("No final state found for the session.")